# ros-nav-and-cv
This project enhances a simulated mobile manipulator by integrating key sensors and a computer vision system using ROS and Gazebo. An IMU sensor is added through a Gazebo plugin to provide acceleration and angular velocity data, which is published to a dedicated ROS topic. To improve the accuracy of this data, it is passed through a filter using the robot localization package, producing a refined output on a separate topic. A GPS sensor is also added to the robot model, simulating global position data. In addition to sensor integration, a computer vision pipeline is implemented to detect stop signs using a pre-trained YOLOv5 model. The model is trained on a small custom dataset and is capable of detecting multiple stop signs in an image or video stream, including real-time input from a webcam. The project demonstrates sensor simulation, data filtering, and object detection within a robotics context.
